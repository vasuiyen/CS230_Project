{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Attention.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMavd0/RlnQlMULn8koZl5u"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_PHwbtWOQ6U"
      },
      "source": [
        "# Necessary packages\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "\n",
        "class Attention():\n",
        "  \"\"\"Attention class.\n",
        "  \n",
        "  Attributes:\n",
        "    - model_parameters:\n",
        "      - task: classificiation or regression\n",
        "      - h_dim: hidden state dimensions\n",
        "      - batch_size: the number of samples in each mini-batch\n",
        "      - epoch: the number of iterations\n",
        "      - learning_rate: learning rate of training\n",
        "  \"\"\"\n",
        "  def __init__(self, model_parameters):\n",
        "\n",
        "    tf.compat.v1.reset_default_graph()\n",
        "    self.task = model_parameters['task']\n",
        "    self.h_dim = model_parameters['h_dim']\n",
        "    self.batch_size = model_parameters['batch_size']\n",
        "    self.epoch = model_parameters['epoch']\n",
        "    self.learning_rate = model_parameters['learning_rate']\n",
        "    \n",
        "    self.save_file_directory = 'tmp/attention/'\n",
        "    \n",
        "    \n",
        "  def process_batch_input_for_RNN(self, batch_input):\n",
        "    \"\"\"Function to convert batch input data to use scan ops of tensorflow.\n",
        "    \n",
        "    Args:\n",
        "      - batch_input: original batch input\n",
        "    \n",
        "    Returns:\n",
        "      - x: batch_input for RNN \n",
        "    \"\"\"\n",
        "    batch_input_ = tf.transpose(batch_input, perm=[2, 0, 1])\n",
        "    x = tf.transpose(batch_input_)\n",
        "    return x\n",
        "\n",
        "\n",
        "  def sample_X(self, m, n):\n",
        "    \"\"\"Sample from the real data (Mini-batch index sampling).\n",
        "    \"\"\"\n",
        "    return np.random.permutation(m)[:n]  \n",
        "  \n",
        "  \n",
        "  def fit(self, x, y):\n",
        "    \"\"\"Train the model.\n",
        "    \n",
        "    Args:\n",
        "      - x: training feature\n",
        "      - y: training label\n",
        "    \"\"\"\n",
        "        \n",
        "    # Basic parameters\n",
        "    no, seq_len, x_dim = x.shape\n",
        "    y_dim = len(y[0, :])\n",
        "    \n",
        "    # Weights for GRU\n",
        "    Wr = tf.Variable(tf.zeros([x_dim, self.h_dim]))\n",
        "    Ur = tf.Variable(tf.zeros([self.h_dim, self.h_dim]))\n",
        "    br = tf.Variable(tf.zeros([self.h_dim]))\n",
        "        \n",
        "    Wu = tf.Variable(tf.zeros([x_dim, self.h_dim]))\n",
        "    Uu = tf.Variable(tf.zeros([self.h_dim, self.h_dim]))\n",
        "    bu = tf.Variable(tf.zeros([self.h_dim]))\n",
        "        \n",
        "    Wh = tf.Variable(tf.zeros([x_dim, self.h_dim]))\n",
        "    Uh = tf.Variable(tf.zeros([self.h_dim, self.h_dim]))\n",
        "    bh = tf.Variable(tf.zeros([self.h_dim]))\n",
        "                \n",
        "    # Weights for attention mechanism \n",
        "    Wa1 = tf.Variable(tf.random.truncated_normal([self.h_dim + x_dim, \n",
        "                                                  self.h_dim], \n",
        "                                                 mean=0, stddev=.01))\n",
        "    Wa2 = tf.Variable(tf.random.truncated_normal([self.h_dim, y_dim], \n",
        "                                                 mean=0, stddev=.01))\n",
        "    ba1 = tf.Variable(tf.random.truncated_normal([self.h_dim], \n",
        "                                                 mean=0, stddev=.01))\n",
        "    ba2 = tf.Variable(tf.random.truncated_normal([y_dim], mean=0, stddev=.01))\n",
        "            \n",
        "    # Weights for output layers\n",
        "    Wo = tf.Variable(tf.random.truncated_normal([self.h_dim, y_dim], \n",
        "                                         mean=0, stddev=.01))\n",
        "    bo = tf.Variable(tf.random.truncated_normal([y_dim], mean=0, stddev=.01))\n",
        "    \n",
        "    # Target\n",
        "    Y = tf.compat.v1.placeholder(tf.float32, [None,1])    \n",
        "    # Input vector with shape[batch, seq, embeddings]\n",
        "    _inputs = tf.compat.v1.placeholder(tf.float32, shape=[None, None, x_dim], \n",
        "                                       name='inputs')\n",
        "    \n",
        "    # Processing inputs to work with scan function\n",
        "    processed_input = self.process_batch_input_for_RNN(_inputs)\n",
        "            \n",
        "    # Initial Hidden States\n",
        "    initial_hidden = _inputs[:, 0, :]\n",
        "    initial_hidden = tf.matmul(initial_hidden, tf.zeros([x_dim, self.h_dim]))\n",
        "        \n",
        " \n",
        "    def GRU(previous_hidden_state, x):\n",
        "      \"\"\"Function for Forward GRU cell.\n",
        "      \n",
        "      Args:\n",
        "        - previous_hidden_state\n",
        "        - x: current input\n",
        "        \n",
        "      Returns:\n",
        "        - current_hidden_state\n",
        "      \"\"\"\n",
        "      # R Gate\n",
        "      r = tf.sigmoid(tf.matmul(x, Wr) + \\\n",
        "                     tf.matmul(previous_hidden_state, Ur) + br)\n",
        "      # U Gate\n",
        "      u = tf.sigmoid(tf.matmul(x, Wu) + \\\n",
        "                     tf.matmul(previous_hidden_state, Uu) + bu)\n",
        "      # Final Memory cell\n",
        "      c = tf.tanh(tf.matmul(x, Wh) + \\\n",
        "                  tf.matmul( tf.multiply(r, previous_hidden_state), Uh) + bh)\n",
        "      # Current Hidden state\n",
        "      current_hidden_state = tf.multiply( (1 - u), previous_hidden_state ) + \\\n",
        "                             tf.multiply( u, c )\n",
        "      return current_hidden_state\n",
        "        \n",
        "    \n",
        "    def get_states():\n",
        "      \"\"\"Function to get the hidden and memory cells after forward pass.\n",
        "      \n",
        "      Returns:\n",
        "        - all_hidden_states\n",
        "      \"\"\"\n",
        "      # Getting all hidden state through time\n",
        "      all_hidden_states = tf.scan(GRU, processed_input, \n",
        "                                  initializer=initial_hidden, name='states')\n",
        "      return all_hidden_states\n",
        "              \n",
        "        \n",
        "    def get_attention(hidden_state):\n",
        "      \"\"\"Function to get attention with the last input.\n",
        "      \n",
        "      Args:\n",
        "        - hidden_states\n",
        "        \n",
        "      Returns:\n",
        "        - e_values\n",
        "      \"\"\"\n",
        "      inputs = tf.concat((hidden_state, processed_input[-1]), axis = 1)\n",
        "      hidden_values = tf.nn.tanh(tf.matmul(inputs, Wa1) + ba1)\n",
        "      e_values = (tf.matmul(hidden_values, Wa2) + ba2)\n",
        "      return e_values\n",
        "        \n",
        "\n",
        "    def get_outputs():\n",
        "      \"\"\"Function for getting output and attention coefficient.\n",
        "      \n",
        "      Returns:\n",
        "        - output: final outputs\n",
        "        - a_values: attention values\n",
        "      \"\"\"\n",
        "      all_hidden_states = get_states()\n",
        "      all_attention = tf.map_fn(get_attention, all_hidden_states)\n",
        "      a_values = tf.nn.softmax(all_attention, axis = 0)\n",
        "      final_hidden_state = tf.einsum('ijk,ijl->jkl', a_values, \n",
        "                                     all_hidden_states)\n",
        "      output = tf.nn.sigmoid(tf.matmul(final_hidden_state[:,0,:], Wo) + bo, \n",
        "                             name='outputs')\n",
        "      return output, a_values   \n",
        "                \n",
        "    # Getting all outputs from rnn\n",
        "    outputs, attention_values = get_outputs()\n",
        "    \n",
        "    # reshape out for sequence_loss\n",
        "    if self.task == 'classification':\n",
        "      loss = tf.reduce_mean(Y * tf.log(outputs + 1e-8) + \\\n",
        "                            (1-Y) * tf.log(1-outputs + 1e-8))\n",
        "    elif self.task == 'regression':\n",
        "      loss = tf.sqrt(tf.reduce_mean(tf.square(outputs - Y)))\n",
        "    \n",
        "    # Optimization\n",
        "    optimizer = tf.compat.v1.train.AdamOptimizer(self.learning_rate)\n",
        "    train = optimizer.minimize(loss)\n",
        "\n",
        "    # Sessions\n",
        "    sess = tf.compat.v1.Session()\n",
        "    sess.run(tf.compat.v1.global_variables_initializer())\n",
        "        \n",
        "    # Training\n",
        "    iteration_per_epoch = int(no/self.batch_size)\n",
        "    iterations = int((self.epoch * no) / self.batch_size)\n",
        "    \n",
        "    for i in range(iterations):\n",
        "      \n",
        "      idx = self.sample_X(no, self.batch_size)\n",
        "      Input = x[idx,:,:]            \n",
        "      _, step_loss = sess.run([train, loss], \n",
        "                              feed_dict={Y: y[idx], _inputs: Input})\n",
        "                \n",
        "      # Print intermediate results\n",
        "      if i % iteration_per_epoch == iteration_per_epoch-1:\n",
        "        print('Epoch: ' + str(int(i/iteration_per_epoch)) + \n",
        "              ', Loss: ' + str(np.round(step_loss, 4)))\n",
        "        \n",
        "    # Reset the directory for saving\n",
        "    if not os.path.exists(self.save_file_directory):\n",
        "      os.makedirs(self.save_file_directory)\n",
        "    else:\n",
        "      shutil.rmtree(self.save_file_directory)\n",
        "  \n",
        "    # Save model\n",
        "    inputs = {'inputs': _inputs}\n",
        "    outputs = {'outputs': outputs}\n",
        "    tf.compat.v1.saved_model.simple_save(sess, self.save_file_directory, \n",
        "                                         inputs, outputs)    \n",
        "        \n",
        "       \n",
        "  def predict(self, test_x):\n",
        "    \"\"\"Prediction with trained model.\n",
        "    \n",
        "    Args:\n",
        "      - test_x: testing features\n",
        "      \n",
        "    Returns:\n",
        "      - test_y_hat: predictions on testing set\n",
        "    \"\"\"\n",
        "    \n",
        "    graph = tf.Graph()\n",
        "    with graph.as_default():\n",
        "      with tf.compat.v1.Session() as sess:\n",
        "        tf.compat.v1.saved_model.loader.load(sess, [tf.saved_model.SERVING], \n",
        "                                             self.save_file_directory)\n",
        "        x = graph.get_tensor_by_name('inputs:0')\n",
        "        outputs = graph.get_tensor_by_name('outputs:0')\n",
        "    \n",
        "        test_y_hat = sess.run(outputs, feed_dict={x: test_x})\n",
        "        \n",
        "    return test_y_hat"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}